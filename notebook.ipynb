{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osanseviero/khipu_workshop/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro \n",
        "\n",
        "### Why Transformers?\n",
        "\n",
        "Deep learning is currently undergoing a period of rapid progress across a wide variety of domains, including: \n",
        "\n",
        "* üìñ Natural language processing\n",
        "* üëÄ Computer vision\n",
        "* üîä Audio\n",
        "* üß¨ Biology\n",
        "* and many more!\n",
        "\n",
        "One of the main drivers of these breakthroughs is the **Transformer** -- a novel **neural network** developed by Google researchers in 2017. \n",
        "\n",
        "Here's a few examples of what Transformers can do:\n",
        "\n",
        "* üíª They can **generate code** as in products like [GitHub Copilot](https://copilot.github.com/), which is based on OpenAI's family of [GPT models](https://huggingface.co/gpt2?text=My+name+is+Clara+and+I+am).\n",
        "* ‚ùì They can be used for **improve search engines**, like [Google did](https://www.blog.google/products/search/search-language-understanding-bert/) with a Transformer called [BERT](https://huggingface.co/bert-base-uncased).\n",
        "* üó£Ô∏è They can **process speech in multiple languages** to perform speech recognition, speech translation, and language identification. For example, Facebook's [XLS-R model](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16) can automatically transcribe audio in one language to another!\n",
        "\n",
        "Training these models **from scratch** involves **a lot of resources**: you need large amounts of compute, data, and days to train for üò±.\n",
        "\n",
        "Fortunately, you don't need to do this in most cases! Thanks to a technique known as **transfer learning**, it is possible to adapt a model that has been trained from scratch (usually called a **pretrained model**), to a variety of downstream tasks. This process is called **fine-tuning** and can typically be carried with a single GPU and a dataset of the size that you're like to find in your university or company.\n",
        "\n",
        "The models that we'll be looking at in this section are all examples of existing fine-tuned models."
      ],
      "metadata": {
        "id": "URpv6AN5hhcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Transformers are coolest kids in town, but how can we use them? If only there was a library that could help us ... oh wait, there is! The [Hugging Face Transformers library](https://github.com/huggingface/transformers) provides a unified API across dozens of Transformer architectures, as well as the means to train models and run inference with them. So to get started, let's install the library with the following command:"
      ],
      "metadata": {
        "id": "VYoQNRhEfaAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers[sentencepiece] datasets evaluate gradio"
      ],
      "metadata": {
        "id": "CBD02hcohkgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest way to learn what Transformers can do is via the `pipeline()` function. This function loads a model from the Hugging Face Hub and takes care of all the preprocessing and postprocessing steps that are needed to convert inputs into predictions."
      ],
      "metadata": {
        "id": "_K43pER4fgB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a basic sentiment analysis task, leveraging a pretrained model from the Hugging Face Hub to categorize the following snippet according to its sentiment (positive or negative):"
      ],
      "metadata": {
        "id": "xr-sOAz-dgeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "renGpjoWd02Z"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Estimado Amazon, la semana pasada ped√≠ una figura de acci√≥n de \\\n",
        "Optimus Prime de su tienda online en Alemania. Desafortunadamente, cuando abr√≠ \\\n",
        "el paquete descubr√≠ con un gran horror que me hab√≠an enviado una figura de acci√≥n de \\\n",
        "Megatron en su lugar. Como enemigo de toda la vida de los Decepticons, espero \\\n",
        "que puedan entender mi dilema. Para resolver el problema, exijo un intercambio \\\n",
        "de Megatron por la figura de Optimus Prime que ped√≠. Adjunto copias de mis \\\n",
        "registros relacionados con esta compra. Espero tener noticias suyas pronto. \\\n",
        "Sinceramente, Bumblebee.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline =  pipeline('text-classification', \n",
        "                              model=\"pysentimiento/robertuito-sentiment-analysis\")"
      ],
      "metadata": {
        "id": "AQHnw0H9hmdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline(text)"
      ],
      "metadata": {
        "id": "1wlEsM7OhpvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline([\"estoy triste\", \"estoy feliz\", \"gran workshop!\"])"
      ],
      "metadata": {
        "id": "OmOvQkl-gJV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now do something a little more sophisticated. Instead of just finding the overall sentiment, let's see if we can extract **entities** such as organizations, locations, or individuals from the text. This task is called named entity recognition, or NER for short. Instead of predicting just a class for the whole text **a class is predicted for each token**, as shown in the example below:"
      ],
      "metadata": {
        "id": "t34z52xBeuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline('ner', model=\"mrm8488/bert-spanish-cased-finetuned-ner\")"
      ],
      "metadata": {
        "id": "Yr6Lu8L8hq4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = ner_pipeline(text, aggregation_strategy=\"simple\")\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "ityYSmlCh52q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in entities:\n",
        "    print(f\"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})\")"
      ],
      "metadata": {
        "id": "u1Pt9QTDh6Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also leverage one of the existing Machine Translation models on the Hugging Face Hub to automatically translate the snippet from English to Spanish -- let's see how it does!"
      ],
      "metadata": {
        "id": "u0MEpIGrhBjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
        "print(outputs[0]['translation_text'])"
      ],
      "metadata": {
        "id": "oAX7IV8lh7wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other useful models on the Hub as well, for instance the \n",
        "[XLM-RoBERTa-large-XNLI-ANLI](https://huggingface.co/vicgalle/xlm-roberta-large-xnli-anli) model, which is a large language model (XLM-RoBERTa-large) that was finetuned over several natural language inference datasets, intended to be use for zero-shot classification in multiple languages:"
      ],
      "metadata": {
        "id": "96Z53Cxxh5BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_classifier = pipeline(\"zero-shot-classification\",\n",
        "                                model=\"vicgalle/xlm-roberta-large-xnli-anli\")"
      ],
      "metadata": {
        "id": "Nyi_LB2xh-hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Alg√∫n d√≠a ir√© a ver el mundo\"\n",
        "classes = ['viaje', 'cocina', 'danza']"
      ],
      "metadata": {
        "id": "XAngwNbsiGcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_classifier(text, classes, multi_label=True)"
      ],
      "metadata": {
        "id": "nbThv3DuiGko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets\n",
        "\n",
        "In this section we'll learn the basics of the `datasets` library."
      ],
      "metadata": {
        "id": "iNSkK1laiVpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load dataset builders from the Hub to show information about datasets without loading the full thing, such as the dataset description:"
      ],
      "metadata": {
        "id": "BVsmi9hGihAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(\"amazon_reviews_multi\")\n",
        "\n",
        "ds_builder.info.description"
      ],
      "metadata": {
        "id": "nLrIRiASifma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as the dataset features (so we know whether this dataset is useful for us, for example)."
      ],
      "metadata": {
        "id": "neNm4-ULinf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_builder.info.features"
      ],
      "metadata": {
        "id": "6YjX4ZWFjLg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've decided that this dataset works for our purpose, we can then load the whole thing"
      ],
      "metadata": {
        "id": "4xmPyFXBi3fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "KrIIMqH1jQLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "RbaxzgiHpgyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"review_body\"]"
      ],
      "metadata": {
        "id": "43xxqa1sjWR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"stars\"]"
      ],
      "metadata": {
        "id": "8Aag9lodjW5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(\"pandas\")\n",
        "df = dataset[\"train\"][:]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FUoVjkdKjdCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"stars\"].value_counts()"
      ],
      "metadata": {
        "id": "bzfBv2ITjigM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.reset_format()"
      ],
      "metadata": {
        "id": "qb5zCi9Ijjhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "NKBMFfRHiK-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to access all of the features of the Hub, you'll need to create an account at https://huggingface.co/ -- you can then create an Access Token and use it to log in directly from the notebook. This is a free service and will allow you to share your models."
      ],
      "metadata": {
        "id": "NJPyzvt1jI1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "USOpAER7q6cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we want to train a binary classifier, we will map the number of stars of each review to a label of 0 or 1. We discard the neutral reviews (3 stars)."
      ],
      "metadata": {
        "id": "lc_F7doRmZEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda x : x[\"stars\"] != 3)\n",
        "\n",
        "def merge_star_ratings(examples):\n",
        "    if examples[\"stars\"] <= 2:\n",
        "        label = 0\n",
        "    else:\n",
        "        label = 1\n",
        "    return {\"labels\": label}\n",
        "\n",
        "dataset = dataset.map(merge_star_ratings)"
      ],
      "metadata": {
        "id": "85g1mYffjp_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we can load a pretrained model (Roberta base) from the Hub and continue fine-tuning it. We first load its tokenizer, which we will use to tokenize the dataset we loaded above:"
      ],
      "metadata": {
        "id": "Fn783Hbojkxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"BSC-TeMU/roberta-base-bne\""
      ],
      "metadata": {
        "id": "x_mmkRQvjyIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "1VSec7zwlmh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what it will look like once a text has been tokenized: "
      ],
      "metadata": {
        "id": "pYl4LAopj8nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"¬°hola, me llamo Omar!\"\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "for token in tokenized_text:\n",
        "  print(token, tokenizer.decode([token]))"
      ],
      "metadata": {
        "id": "UTdTmGvploit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
        "encoded_text"
      ],
      "metadata": {
        "id": "AbnwiWREszit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then write a tokenization function and use it to tokenize the whole dataset:"
      ],
      "metadata": {
        "id": "3fIPcpRWkDV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_reviews(examples):\n",
        "  return tokenizer(examples[\"review_body\"], truncation=True)\n",
        "\n",
        "columns = dataset[\"train\"].column_names\n",
        "columns.remove(\"labels\")\n",
        "encoded_dataset = dataset.map(tokenize_reviews, batched=True, remove_columns=columns)"
      ],
      "metadata": {
        "id": "tFzfSWDbiHsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset"
      ],
      "metadata": {
        "id": "fYdJ90T2lW0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "Aw7rMSJ3sgBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then load the pretrained model itself and define how many labels we want it to have:"
      ],
      "metadata": {
        "id": "oiZb_mwKkLrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "2MQjuYoDlhKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**encoded_text)\n",
        "outputs"
      ],
      "metadata": {
        "id": "lG2M2G-CswcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then define the training arguments, as well as the model ID for the hub (change it to your own username!)"
      ],
      "metadata": {
        "id": "b6k8hqUclXba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "batch_size = 16\n",
        "num_train_epochs=2\n",
        "num_train_samples = 1000\n",
        "train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(num_train_samples))\n",
        "logging_steps = len(train_dataset) // (2 * batch_size * num_train_epochs)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    num_train_epochs=num_train_epochs,     \n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\", \n",
        "    logging_steps=logging_steps,\n",
        "    push_to_hub=True,\n",
        "    push_to_hub_model_id=f\"khipu-finetuned-amazon_reviews_multi\"\n",
        ")"
      ],
      "metadata": {
        "id": "yhB0XF-Ulu1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting model training, we'll define the way to compute metrics with `evaluate`, a library for doing systematic and principled evaluation of ML models (which means that you can do all sorts of model evaluations without writing custom scripts!)"
      ],
      "metadata": {
        "id": "ROQSQvzFlmEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this purpose, we will only be evaluating our model's accuracy, but keep in mind that there are tons of other metrics you can use (see the full list [here](https://huggingface.co/metrics)!)"
      ],
      "metadata": {
        "id": "CT1ThxRtl8FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "metric"
      ],
      "metadata": {
        "id": "4cc_vstPlx5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "IEm5Ppgql3CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's start training! üöÄ"
      ],
      "metadata": {
        "id": "HJ9LaaghmYd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "p8Ehvt02maSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0BKMRUwfmac2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then push both the trained model and the tokenizer to the Hub so we can use them later"
      ],
      "metadata": {
        "id": "-I4BcKlwmfRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "Y9bKN2x4p5ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And... magic!‚ú® We can directly use our finetuned model in a `pipeline`! üòÆ"
      ],
      "metadata": {
        "id": "sTxLiO8Em1az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline \n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"osanseviero/khipu-finetuned-amazon_reviews_multi\", return_all_scores=True)"
      ],
      "metadata": {
        "id": "Be8_ZM_zt_SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then take a new review (that's not from the Yelp dataset) and feed it through our finetuned model, to get its sentiment:"
      ],
      "metadata": {
        "id": "MqfHE3mznYGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Estimado Amazon, la semana pasada ped√≠ una figura de acci√≥n de \\\n",
        "Optimus Prime de su tienda online en Alemania. Desafortunadamente, cuando abr√≠ \\\n",
        "el paquete descubr√≠ con un gran horror que me hab√≠an enviado una figura de acci√≥n de \\\n",
        "Megatron en su lugar. Como enemigo de toda la vida de los Decepticons, espero \\\n",
        "que puedan entender mi dilema. Para resolver el problema, exijo un intercambio \\\n",
        "de Megatron por la figura de Optimus Prime que ped√≠. Adjunto copias de mis \\\n",
        "registros relacionados con esta compra. Espero tener noticias suyas pronto. \\\n",
        "Sinceramente, Bumblebee.\"\"\"\n",
        "pipe(text)"
      ],
      "metadata": {
        "id": "RTu292nAuD9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "JF2Bmocpo1nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at creating easy, interactive ML demos using [Gradio](https://gradio.app/)!"
      ],
      "metadata": {
        "id": "5UAfiRbqnx2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can write any custom function and then call it using `gr.Interace`, specifying what kind of elements it takes as input and produces as output -- e.g. text, image, sound, etc. \n",
        "This automatically creates a simple, interactive interface, and launches it directly in your notebook:"
      ],
      "metadata": {
        "id": "TPmER_E3oy4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return f\"Hello {name}\"\n",
        "\n",
        "gr.Interface(\n",
        "    greet,\n",
        "    \"text\",\n",
        "    \"text\",\n",
        "    title=\"Greet!\",\n",
        "    allow_flagging=False\n",
        ").launch()"
      ],
      "metadata": {
        "id": "CxSrXSRjo1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also call the pipeline we fine-tuned above in this way, and also wrap it in an interface, which would allow people to use it in an interactive way, feeding it text and getting the labels out:"
      ],
      "metadata": {
        "id": "6gdeDUuapKrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict(input):\n",
        "    res = {}\n",
        "    for pred in pipe(input)[0]:\n",
        "      res[pred[\"label\"]] = pred[\"score\"]\n",
        "    return res\n",
        "\n",
        "gr.Interface(\n",
        "    predict,\n",
        "    \"text\",\n",
        "    \"label\",\n",
        "    title=\"Classify!\",\n",
        "    examples=[[text]],\n",
        "    allow_flagging=False\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "BPcsGNSQmcZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBKrUvREvPG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}